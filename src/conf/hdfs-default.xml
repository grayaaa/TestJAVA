<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->
<!-- Do not modify this file directly.  Instead, copy entries that you -->
<!-- wish to modify from this file into hdfs-site.xml and change them -->
<!-- there.  If hdfs-site.xml does not already exist, create it.      -->
<configuration> 
   <property> 
      <name>dfs.namenode.name.dir</name>  
      <defaultValue>file://${hadoop.tmp.dir}/dfs/name</defaultValue>  
      <description>Determines where on the local filesystem the DFS name node should store the name table.</description>  
      <type>基础</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.checkpoint.dir</name>  
      <defaultValue>file://${hadoop.tmp.dir}/dfs/namesecondary</defaultValue>  
      <description>Determines where on the local filesystem the DFS snn should store the temporary images to merge.</description>  
      <type>基础</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.journalnode.edits.dir</name>  
      <defaultValue>file://${hadoop.tmp.dir}/dfs/sharelog</defaultValue>  
      <description>Determines where on the local filesystem the DFS journalnode node should store the share log.</description>  
      <type>基础</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.permissions.enabled</name>  
      <defaultValue>true</defaultValue>  
      <description>If "true", enable permission checking in HDFS. If "false", permission checking is turned off, but all others is unchanged.</description>  
      <type>基础</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.permissions.superusergroup</name>  
      <defaultValue>supergroup</defaultValue>  
      <description>The name of the group of super-users.</description>  
      <type>基础</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.datanode.data.dir</name>  
      <defaultValue>file://${hadoop.tmp.dir}/dfs/data</defaultValue>  
      <description>Determines where on the local filesystem an DFS data node should store its blocks.</description>  
      <type>基础</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.replication</name>  
      <defaultValue>3</defaultValue>  
      <description>Default block replication.</description>  
      <type>基础</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.blocksize</name>  
      <defaultValue>67108864</defaultValue>  
      <description>The default block size for new files, in bytes.</description>  
      <type>基础</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.heartbeat.interval</name>  
      <defaultValue>3</defaultValue>  
      <description>Determines datanode heartbeat interval in seconds.</description>  
      <type>基础</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.webhdfs.enabled</name>  
      <defaultValue>false</defaultValue>  
      <description>Enable WebHDFS (REST API) in Namenodes and Datanodes.</description>  
      <type>基础</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.https.enable</name>  
      <defaultValue>false</defaultValue>  
      <description>Decide if HTTPS(SSL) is supported on HDFS</description>  
      <type>基础</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>hadoop.hdfs.configuration.version</name>  
      <defaultValue>1</defaultValue>  
      <description>version of this configuration file</description>  
      <type>基础</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.logging.level</name>  
      <defaultValue>info</defaultValue>  
      <description>The logging level for dfs namenode.</description>  
      <type>基础</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.datanode.handler.count</name>  
      <defaultValue>10</defaultValue>  
      <description>The number of server threads for the datanode.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.client.https.need-auth</name>  
      <defaultValue>false</defaultValue>  
      <description>Whether SSL client certificate authentication is required</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.https.server.keystore.resource</name>  
      <defaultValue>ssl-server.xml</defaultValue>  
      <description>Resource file from which ssl server keystore information will be extracted</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.client.https.keystore.resource</name>  
      <defaultValue>ssl-client.xml</defaultValue>  
      <description>Resource file from which ssl client keystore information will be extracted</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.datanode.dns.interface</name>  
      <defaultValue>default</defaultValue>  
      <description>The name of the Network Interface from which a data node should report its IP address.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.datanode.dns.nameserver</name>  
      <defaultValue>default</defaultValue>  
      <description>The host name or IP address of the name server (DNS)</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.replication.considerLoad</name>  
      <defaultValue>true</defaultValue>  
      <description>Decide if chooseTarget considers the target's load or not</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.default.chunk.view.size</name>  
      <defaultValue>32768</defaultValue>  
      <description>The number of bytes to view for a file on the browser.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.datanode.du.reserved</name>  
      <defaultValue>0</defaultValue>  
      <description>Reserved space in bytes per volume. Always leave this much space free for non dfs use.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.name.dir.restore</name>  
      <defaultValue>false</defaultValue>  
      <description>Set to true to enable NameNode to attempt recovering a previously failed dfs.namenode.name.dir.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.fs-limits.max-component-length</name>  
      <defaultValue>0</defaultValue>  
      <description>Defines the maximum number of characters in each component of a path. A value of 0 will disable the check.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.fs-limits.max-directory-items</name>  
      <defaultValue>0</defaultValue>  
      <description>Defines the maximum number of items that a directory may contain. A value of 0 will disable the check.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.cluster.administrators</name>  
      <defaultValue>ACL for the admins</defaultValue>  
      <description>This configuration is used to control who can access the default servlets in the namenode, etc.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.block.access.token.enable</name>  
      <defaultValue>false</defaultValue>  
      <description>If "true", access tokens are used as capabilities for accessing datanodes. If "false", no access tokens are checked on accessing datanodes.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.block.access.key.update.interval</name>  
      <defaultValue>600</defaultValue>  
      <description>Interval in minutes at which namenode updates its access keys.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.block.access.token.lifetime</name>  
      <defaultValue>600</defaultValue>  
      <description>The lifetime of access tokens in minutes.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.datanode.data.dir.perm</name>  
      <defaultValue>700</defaultValue>  
      <description>Permissions for the directories on on the local filesystem</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.replication.max</name>  
      <defaultValue>512</defaultValue>  
      <description>Maximal block replication.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.replication.min</name>  
      <defaultValue>1</defaultValue>  
      <description>Minimal block replication.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.client.block.write.retries</name>  
      <defaultValue>3</defaultValue>  
      <description>The number of retries for writing blocks to the data nodes, before we signal failure to the application.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.client.block.write.replace-datanode-on-failure.enable</name>  
      <defaultValue>true</defaultValue>  
      <description/>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.client.block.write.replace-datanode-on-failure.policy</name>  
      <defaultValue>DEFAULT</defaultValue>  
      <description>This property is used only if the value of dfs.client.block.write.replace-datanode-on-failure.enable is true.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.blockreport.intervalMsec</name>  
      <defaultValue>21600000</defaultValue>  
      <description>Determines block reporting interval in milliseconds.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.blockreport.initialDelay</name>  
      <defaultValue>0</defaultValue>  
      <description>Delay for first block report in seconds.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.datanode.directoryscan.interval</name>  
      <defaultValue>21600</defaultValue>  
      <description>Interval in seconds for Datanode to scan data directories and reconcile the difference between blocks in memory and on the disk.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.datanode.directoryscan.threads</name>  
      <defaultValue>1</defaultValue>  
      <description>How many threads should the threadpool used to compile reports for volumes in parallel have.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.handler.count</name>  
      <defaultValue>10</defaultValue>  
      <description>The number of server threads for the namenode.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.safemode.threshold-pct</name>  
      <defaultValue>0.999f</defaultValue>  
      <description>Values greater than 1 will make safe mode permanent.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.safemode.min.datanodes</name>  
      <defaultValue>0</defaultValue>  
      <description>Specifies the number of datanodes that must be considered alive before the name node exits safemode.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.safemode.extension</name>  
      <defaultValue>30000</defaultValue>  
      <description>Determines extension of safe mode in milliseconds after the threshold level is reached.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.datanode.balance.bandwidthPerSec</name>  
      <defaultValue>1048576</defaultValue>  
      <description>Specifies the maximum amount of bandwidth</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.max.objects</name>  
      <defaultValue>0</defaultValue>  
      <description>The maximum number of files, directories and blocks dfs supports.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.decommission.interval</name>  
      <defaultValue>30</defaultValue>  
      <description>Namenode periodicity in seconds to check if decommission is complete.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.decommission.nodes.per.interval</name>  
      <defaultValue>5</defaultValue>  
      <description>The number of nodes namenode checks if decommission is complete in each dfs.namenode.decommission.interval.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.replication.interval</name>  
      <defaultValue>3</defaultValue>  
      <description>The periodicity in seconds with which the namenode computes repliaction work for datanodes.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.accesstime.precision</name>  
      <defaultValue>3600000</defaultValue>  
      <description>The access time for HDFS file is precise upto this value.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.datanode.plugins</name>  
      <value/>  
      <description>Comma-separated list of datanode plug-ins to be activated.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.plugins</name>  
      <value/>  
      <description>Comma-separated list of namenode plug-ins to be activated.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.stream-buffer-size</name>  
      <defaultValue>4096</defaultValue>  
      <description>The size of buffer to stream files.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.bytes-per-checksum</name>  
      <defaultValue>512</defaultValue>  
      <description>The number of bytes per checksum. Must not be larger than dfs.stream-buffer-size</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.client-write-packet-size</name>  
      <defaultValue>65536</defaultValue>  
      <description>Packet size for clients to write</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.checkpoint.dir</name>  
      <defaultValue>file://${hadoop.tmp.dir}/dfs/namesecondary</defaultValue>  
      <description>Determines where on the local filesystem the DFS secondary name node should store the temporary images to merge.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.checkpoint.edits.dir</name>  
      <defaultValue>${dfs.namenode.checkpoint.dir}</defaultValue>  
      <description>Determines where on the local filesystem the DFS secondary name node should store the temporary edits to merge.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.checkpoint.period</name>  
      <defaultValue>3600</defaultValue>  
      <description>The number of seconds between two periodic checkpoints.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.checkpoint.txns</name>  
      <defaultValue>40000</defaultValue>  
      <description>The Secondary NameNode or CheckpointNode will create a checkpoint of the namespace every transactions</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.checkpoint.check.period</name>  
      <defaultValue>60</defaultValue>  
      <description>query the number of uncheckpointed transactions.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.num.checkpoints.retained</name>  
      <defaultValue>2</defaultValue>  
      <description>The number of image checkpoint files that will be retained by the NameNode and Secondary NameNode.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.num.extra.edits.retained</name>  
      <defaultValue>1000000</defaultValue>  
      <description>The number of extra transactions which should be retained beyond what is minimally necessary for a NN restart.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.max.extra.edits.segments.retained</name>  
      <defaultValue>10000</defaultValue>  
      <description>The maximum number of extra edit log segments which should be retained beyond what is minimally necessary for a NN restart.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.delegation.key.update-interval</name>  
      <defaultValue>86400000</defaultValue>  
      <description>The update interval for master key for delegation tokens in the namenode in milliseconds.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.delegation.token.max-lifetime</name>  
      <defaultValue>604800000</defaultValue>  
      <description>The maximum lifetime in milliseconds for which a delegation token is valid.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.delegation.token.renew-interval</name>  
      <defaultValue>86400000</defaultValue>  
      <description>The renewal interval for delegation token in milliseconds.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.datanode.failed.volumes.tolerated</name>  
      <defaultValue>0</defaultValue>  
      <description>The number of volumes that are allowed to fail before a datanode stops offering service.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.image.compress</name>  
      <defaultValue>false</defaultValue>  
      <description>Should the dfs image be compressed?</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.image.compression.codec</name>  
      <defaultValue>org.apache.hadoop.io.compress.DefaultCodec</defaultValue>  
      <description>This has to be a codec defined in io.compression.codecs.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.image.transfer.bandwidthPerSec</name>  
      <defaultValue>0</defaultValue>  
      <description>Specifies the maximum amount of bandwidth that can be utilized for image transfer in term of the number of bytes per second.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.support.allow.format</name>  
      <defaultValue>true</defaultValue>  
      <description>You may consider setting this to false for any production cluster, to avoid any possibility of formatting a running DFS.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.datanode.max.transfer.threads</name>  
      <defaultValue>4096</defaultValue>  
      <description>Specifies the maximum number of threads to use for transferring data in and out of the DN.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.datanode.readahead.bytes</name>  
      <defaultValue>4193404</defaultValue>  
      <description>This configuration specifies the number of bytes ahead of the current read position which the datanode will attempt to read ahead.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.datanode.drop.cache.behind.reads</name>  
      <defaultValue>false</defaultValue>  
      <description>This may improve performance for some workloads by freeing buffer cache spage usage for more cacheable data.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.datanode.drop.cache.behind.writes</name>  
      <defaultValue>false</defaultValue>  
      <description>If the Hadoop native libraries are not available, this configuration has no effect.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.datanode.sync.behind.writes</name>  
      <defaultValue>false</defaultValue>  
      <description/>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.client.failover.max.attempts</name>  
      <defaultValue>15</defaultValue>  
      <description>The number of client failover attempts that should be made before the failover is considered failed.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.client.failover.sleep.base.millis</name>  
      <defaultValue>500</defaultValue>  
      <description>This option specifies the base value used in the failover calculation.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.client.failover.sleep.max.millis</name>  
      <defaultValue>15000</defaultValue>  
      <description>This option specifies the maximum value to wait between failovers.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.client.failover.connection.retries</name>  
      <defaultValue>0</defaultValue>  
      <description>Expert only. Indicates the number of retries a failover IPC client will make to establish a server connection.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.client.failover.connection.retries.on.timeouts</name>  
      <defaultValue>0</defaultValue>  
      <description>Expert only. The number of retry attempts a failover IPC client will make on socket timeout when establishing a server connection.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.ha.log-roll.period</name>  
      <defaultValue>120</defaultValue>  
      <description>How often, in seconds, the StandbyNode should ask the active to roll edit logs.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.ha.tail-edits.period</name>  
      <defaultValue>60</defaultValue>  
      <description>How often, in seconds, the StandbyNode should check for new finalized log segments in the shared edits log.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.support.append</name>  
      <defaultValue>true</defaultValue>  
      <description>Does HDFS allow appends to files?</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.client.use.datanode.hostname</name>  
      <defaultValue>false</defaultValue>  
      <description>Whether clients should use datanode hostnames when connecting to datanodes.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.datanode.use.datanode.hostname</name>  
      <defaultValue>false</defaultValue>  
      <description>Whether datanodes should use datanode hostnames when connecting to other datanodes for data transfer.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.client.local.interfaces</name>  
      <value/>  
      <description>A comma separated list of network interface names to use for data transfer between the client and datanodes.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.kerberos.internal.spnego.principal</name>  
      <defaultValue>${dfs.web.authentication.kerberos.principal}</defaultValue>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.secondary.namenode.kerberos.internal.spnego.principal</name>  
      <defaultValue>${dfs.web.authentication.kerberos.principal}</defaultValue>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.invalidate.work.pct.per.iteration</name>  
      <defaultValue>0.32f</defaultValue>  
      <description/>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.replication.work.multiplier.per.iteration</name>  
      <defaultValue>2</defaultValue>  
      <description>The actual number is obtained by multiplying this multiplier with the total number of live nodes in the cluster.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>hadoop.fuse.connection.timeout</name>  
      <defaultValue>300</defaultValue>  
      <description>The minimum number of seconds that we'll cache libhdfs connection objects in fuse_dfs.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>hadoop.fuse.timer.period</name>  
      <defaultValue>5</defaultValue>  
      <description>The number of seconds between cache expiry checks in fuse_dfs. quickly.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.metrics.percentiles.intervals</name>  
      <value/>  
      <description>Comma-delimited set of integers denoting the desired rollover intervals for percentile latency metrics on the Namenode and Datanode.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.encrypt.data.transfer</name>  
      <defaultValue>false</defaultValue>  
      <description>Whether or not actual block data that is read/written from/to HDFS should be encrypted on the wire.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.encrypt.data.transfer.algorithm</name>  
      <value/>  
      <description>This value may be set to either "3des" or "rc4".</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>  
      <defaultValue>true</defaultValue>  
      <description>Boolean which enables backend datanode-side support for the experimental getFileVBlockStorageLocations API.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.client.file-block-storage-locations.num-threads</name>  
      <defaultValue>10</defaultValue>  
      <description>Number of threads used for making parallel RPCs in DistributedFileSystem#getFileBlockStorageLocations().</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.client.file-block-storage-locations.timeout</name>  
      <defaultValue>60</defaultValue>  
      <description>Timeout for the parallel RPCs made in DistributedFileSystem#getFileBlockStorageLocations().</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.audit.loggers</name>  
      <defaultValue>default</defaultValue>  
      <description>List of classes implementing audit loggers that will receive audit events.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.domain.socket.path</name>  
      <defaultValue>/var/run/hadoop-hdfs/dn._PORT</defaultValue>  
      <description>This is a path to a UNIX domain socket that will be used for communication between the DataNode and local HDFS clients.</description>  
      <type>高级</type>  
      <source>hdfs-site.xml</source> 
   </property>  
   <property> 
      <name>dfs.namenode.edits.journal-plugin.qjournal</name>  
      <defaultValue>org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager</defaultValue>  
      <description/>  
      <type>高级</type> 
   </property> 
</configuration>
